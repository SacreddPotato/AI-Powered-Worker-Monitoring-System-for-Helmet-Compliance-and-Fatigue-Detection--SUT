<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>FAQ — Technical Details</title>
<link rel="stylesheet" href="styles.css">
<link rel="icon" type="image/x-icon" href="assets/favicon.ico">
</head>
<body>
    <div id="sidebarOverlay"></div>

<div class="page-simple">
<header class="mini-header">
<a href="index.html" class="back">← Back</a>
<h1>Frequently Asked Questions</h1>
</header>

<main class="faq-area">
<aside class="faq-questions">
<ul>
<li class="question" data-answer="ans1">What dataset did you use?</li>
<li class="question" data-answer="ans2">What models are running?</li>
<li class="question" data-answer="ans3">Can I test it with my own video?</li>
</ul>
</aside>

<section class="faq-answer card" id="faqAnswer">
<p>Select a question to see the technical details.</p>

<div id="ans1" class="hidden">
<h4>What dataset did you use?</h4>
<p>We used the <a href="https://www.kaggle.com/datasets/rishab260/uta-reallife-drowsiness-dataset"><strong>University of Texas' Fatigue Dataset</strong></a> specifically for this project. As the team thought this had the perfect balance of real-world conditions and comprehensive annotations.</p>
<p>We also used readily available helmet datasets from public sources to train our helmet detection models.</p>
</div>

<div id="ans2" class="hidden">
<h4>What models are running?</h4>
<p>For Helmet Detection, we use a fine-tuned <strong>YOLOv8</strong> model. For Fatigue Detection, we implemented a hybrid system using <strong>Dlib</strong> for geometric landmarks (EAR/MAR) and a <strong>Swin Transformer</strong> for visual feature extraction.</p>
</div>

<div id="ans3" class="hidden">
<h4>Can I test it with my own video?</h4>
<p>Yes! On the specific demo pages (Helmet or Fatigue), you can upload a local video file to run the inference pipeline on your own footage.</p>
</div>

</section>
</main>
</div>

<script src="app.js"></script>
</body>
</html>